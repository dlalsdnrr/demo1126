from __future__ import annotations

import os
import io
import base64
import tempfile
import time
import difflib
import subprocess
import json
import requests
from typing import Optional, Dict, Any

from flask import Blueprint, jsonify, request
import google.generativeai as genai

from config import GEMINI_API_KEY, WEATHER_API_KEY

# ============================================================================
# API ë° ëª¨ë“ˆ ì´ˆê¸°í™”
# ============================================================================

# --- Gemini API ---
GEMINI_AVAILABLE = False
if GEMINI_API_KEY:
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        GEMINI_AVAILABLE = True
        print("âœ“ Gemini API ì„¤ì • ì™„ë£Œ")
    except Exception as e:
        print(f"âœ— Gemini API ì„¤ì • ì‹¤íŒ¨: {e}")
else:
    print("âœ— Gemini API Key ì—†ìŒ (GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ í•„ìš”)")

# --- STT: Faster Whisper ---
STT_AVAILABLE = False
WHISPER_MODEL = None
try:
    from faster_whisper import WhisperModel
    STT_AVAILABLE = True
    print("âœ“ Faster Whisper STT ë¡œë“œ ì„±ê³µ")
except ImportError:
    print("âœ— faster-whisper ë¯¸ì„¤ì¹˜. ì‹¤í–‰: pip install faster-whisper")
    WhisperModel = None
except Exception as e:
    print(f"âœ— Faster Whisper ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    WhisperModel = None

# --- ì˜¤ë””ì˜¤ ì²˜ë¦¬: ffmpeg ---
FFMPEG_AVAILABLE = False
FFMPEG_PATH = None
def find_ffmpeg():
    global FFMPEG_PATH, FFMPEG_AVAILABLE
    env_path = os.environ.get("FFMPEG_PATH")
    if env_path and os.path.exists(env_path):
        FFMPEG_PATH = env_path
        FFMPEG_AVAILABLE = True
        return True
    try:
        result = subprocess.run(["where" if os.name == "nt" else "which", "ffmpeg"], capture_output=True, text=True, check=True)
        path = result.stdout.strip().split('\n')[0]
        if path and os.path.exists(path):
            FFMPEG_PATH = path
            FFMPEG_AVAILABLE = True
            return True
    except: pass
    return False

if find_ffmpeg():
    print(f"âœ“ ffmpeg ì°¾ìŒ: {FFMPEG_PATH}")
else:
    print("âœ— ffmpeg ì—†ìŒ. ë‹¤ìš´ë¡œë“œ: https://www.gyan.dev/ffmpeg/builds/")

# --- TTS: gTTS & edge-tts ---
GTTS_AVAILABLE = False
try:
    from gtts import gTTS
    GTTS_AVAILABLE = True
    print("âœ“ gTTS ë¡œë“œ ì„±ê³µ")
except:
    print("âœ— gTTS ë¯¸ì„¤ì¹˜. ì‹¤í–‰: pip install gTTS")
    gTTS = None

EDGE_TTS_AVAILABLE = False
try:
    import edge_tts
    import asyncio
    EDGE_TTS_AVAILABLE = True
    print("âœ“ edge-tts ë¡œë“œ ì„±ê³µ")
except:
    print("â„¹ edge-tts ë¯¸ì‚¬ìš© (gTTSë¡œ ëŒ€ì²´)")
    edge_tts, asyncio = None, None
    
# ============================================================================
# ì™¸ë¶€ ì„œë¹„ìŠ¤ í˜¸ì¶œ (ë‚ ì”¨)
# ============================================================================

def get_yongin_weather() -> Optional[Dict[str, Any]]:
    if not WEATHER_API_KEY:
        print("âœ— ë‚ ì”¨ API Key ì—†ìŒ (WEATHER_API_KEY í™˜ê²½ë³€ìˆ˜ í•„ìš”)")
        return None
    
    # ìš©ì¸ì‹œì²­ ì¢Œí‘œ
    lat, lon = 37.2215, 127.1873
    url = f"https://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={WEATHER_API_KEY}&units=metric&lang=kr"
    
    try:
        response = requests.get(url, timeout=5)
        response.raise_for_status()
        data = response.json()
        # í•„ìš”í•œ ì •ë³´ë§Œ ê°„ì¶”ë¦¬ê¸°
        return {
            "ìƒíƒœ": data["weather"][0]["description"],
            "ì˜¨ë„": f"{data['main']['temp']:.1f}Â°C",
            "ì²´ê°ì˜¨ë„": f"{data['main']['feels_like']:.1f}Â°C",
            "ìŠµë„": f"{data['main']['humidity']}%",
            "í’ì†": f"{data['wind']['speed']:.1f}m/s"
        }
    except Exception as e:
        print(f"âœ— ë‚ ì”¨ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return None

# ============================================================================
# í•µì‹¬ ê¸°ëŠ¥ (STT, TTS, ì˜¤ë””ì˜¤ ë³€í™˜)
# ============================================================================

def convert_audio_to_wav(input_bytes: bytes) -> Optional[bytes]:
    """ffmpegë¡œ ì˜¤ë””ì˜¤ë¥¼ 16kHz mono WAVë¡œ ë³€í™˜"""
    if not FFMPEG_AVAILABLE:
        print("âœ— ffmpeg ì—†ìŒ")
        return None
    
    input_file = None
    output_file = None
    
    try:
        # ì…ë ¥ ì„ì‹œ íŒŒì¼
        with tempfile.NamedTemporaryFile(delete=False, suffix=".webm") as f:
            f.write(input_bytes)
            input_file = f.name
        
        # ì¶œë ¥ ì„ì‹œ íŒŒì¼
        output_file = tempfile.mktemp(suffix=".wav")
        
        # ffmpeg ë³€í™˜
        cmd = [
            FFMPEG_PATH,
            "-i", input_file,
            "-ar", "16000",  # 16kHz
            "-ac", "1",       # mono
            "-sample_fmt", "s16",  # 16-bit
            "-f", "wav",
            "-y",  # ë®ì–´ì“°ê¸°
            output_file
        ]
        
        result = subprocess.run(
            cmd,
            capture_output=True,
            check=True
        )
        
        # ì¶œë ¥ íŒŒì¼ ì½ê¸°
        with open(output_file, "rb") as f:
            return f.read()
            
    except Exception as e:
        print(f"âœ— ffmpeg ë³€í™˜ ì‹¤íŒ¨: {e}")
        return None
    finally:
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if input_file and os.path.exists(input_file):
            try:
                os.remove(input_file)
            except:
                pass
        if output_file and os.path.exists(output_file):
            try:
                os.remove(output_file)
            except:
                pass

def speak_gtts(text: str) -> Optional[str]:
    """gTTSë¡œ ìŒì„± í•©ì„± í›„ base64 ë°˜í™˜"""
    if not GTTS_AVAILABLE:
        return None
    
    try:
        print(f"â†’ gTTS ìƒì„±: {text[:30]}...")
        buffer = io.BytesIO()
        tts = gTTS(text=text, lang='ko')
        tts.write_to_fp(buffer)
        buffer.seek(0)
        return base64.b64encode(buffer.read()).decode('utf-8')
    except Exception as e:
        print(f"âœ— gTTS ì‹¤íŒ¨: {e}")
        return None

def speak_edge_tts(text: str) -> Optional[str]:
    """edge-ttsë¡œ ìŒì„± í•©ì„± í›„ base64 ë°˜í™˜"""
    if not EDGE_TTS_AVAILABLE:
        return None
    
    try:
        print(f"â†’ edge-tts ìƒì„±: {text[:30]}...")
        
        async def generate():
            communicate = edge_tts.Communicate(text, "ko-KR-SunHiNeural", rate="+10%")
            audio_data = b""
            async for chunk in communicate.stream():
                if chunk["type"] == "audio":
                    audio_data += chunk["data"]
            
            return base64.b64encode(audio_data).decode('utf-8')
        
        # ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(generate())
        
    except Exception as e:
        print(f"âœ— edge-tts ì‹¤íŒ¨: {e}")
        return None

def get_tts_audio(text: str) -> Optional[str]:
    """TTS ì˜¤ë””ì˜¤ ìƒì„± (edge-tts ìš°ì„ , ì‹¤íŒ¨ ì‹œ gTTS)"""
    # 1ìˆœìœ„: edge-tts
    result = speak_edge_tts(text)
    if result:
        return result
    
    # 2ìˆœìœ„: gTTS
    result = speak_gtts(text)
    if result:
        return result
    
    print("âœ— ëª¨ë“  TTS ì—”ì§„ ì‹¤íŒ¨")
    return None

def load_whisper_model() -> Optional[Any]:
    """Faster Whisper ëª¨ë¸ ì´ˆê¸°í™”"""
    global WHISPER_MODEL, STT_AVAILABLE
    
    if not STT_AVAILABLE:
        return None
    
    if WHISPER_MODEL is None:
        try:
            model_name = "small"
            print(f"â†’ Whisper ëª¨ë¸ ë¡œë”© ì¤‘ ({model_name})...")
            WHISPER_MODEL = WhisperModel(
                model_name,
                device="cpu",
                compute_type="int8",
                cpu_threads=4
            )
            print("âœ“ Whisper ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âœ— Whisper ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            STT_AVAILABLE = False
            WHISPER_MODEL = None
    
    return WHISPER_MODEL

# ============================================================================
# VoiceAssistant í´ë˜ìŠ¤
# ============================================================================

class VoiceAssistant:
    def __init__(self):
        self.whisper_model = load_whisper_model()
        self.gemini_model = genai.GenerativeModel('gemini-2.5-flash') if GEMINI_AVAILABLE else None
        
# ------------------------------------------------------------------
        # ğŸ”´ [ìˆ˜ì •ëœ ë¶€ë¶„ ì‹œì‘] 
        # 1. ë°ì´í„° ë¡œë“œ ë° í‰íƒ„í™” (Flattening)
        # ------------------------------------------------------------------
        self.PLAYERS_DATA = {}
        self.PLAYER_ALIASES = {}
        
        try:
            with open("kbo_data.json", "r", encoding="utf-8") as f:
                raw_data = json.load(f)
                
                # (1) ê³„ì¸µí˜• ë°ì´í„°(íŒ€>í¬ì§€ì…˜>ì„ ìˆ˜)ë¥¼ 1ì°¨ì›ìœ¼ë¡œ í´ê¸°
                if "PLAYERS_DATA" in raw_data:
                    teams = raw_data["PLAYERS_DATA"]
                    for team_name, positions in teams.items():
                        for position, players in positions.items():
                            for name, stats in players.items():
                                # ë°ì´í„°ì— íŒ€/í¬ì§€ì…˜ ì •ë³´ ì£¼ì…
                                stats["íŒ€"] = team_name
                                stats["í¬ì§€ì…˜"] = position
                                self.PLAYERS_DATA[name] = stats

                # (2) íŒŒì¼ì— ìˆëŠ” ë³„ëª… ê°€ì ¸ì˜¤ê¸°
                self.PLAYER_ALIASES.update(raw_data.get("PLAYER_ALIASES", {}))
                
                print(f"âœ“ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: ì´ {len(self.PLAYERS_DATA)}ëª… (KBO_DATA)")

        except FileNotFoundError:
            print("âš ï¸ kbo_data.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âœ— ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

        # ------------------------------------------------------------------
        # 2. í‚¤ì›Œë“œ ì •ì˜ (ìš”ì²­í•˜ì‹  ëŒ€ë¡œ ë³µêµ¬ ì™„ë£Œ! âœ…)
        # ------------------------------------------------------------------
        self.KEYWORDS = {
            # --- íƒ€ì ê¸°ë¡ ---
            "íƒ€ìœ¨": ["íƒ€ìœ¨", "íƒ€ì´ìœ¨", "AVG", "ì—ë²„ë¦¬ì§€"],
            "í™ˆëŸ°": ["í™ˆëŸ°", "í™ˆë¡¬", "HR", "ë‹´ì¥"],
            "ì•ˆíƒ€": ["ì•ˆíƒ€", "ì•ˆ íƒ€", "H"],
            "íƒ€ì ": ["íƒ€ì ", "RBI", "ì ìˆ˜ ë‚¸ ê±°", "ë¶ˆëŸ¬ë“¤ì¸ ê±°"],
            "ë“ì ": ["ë“ì ", "R", "ë“¤ì–´ì˜¨ ê±°", "í™ˆì¸"],
            "2ë£¨íƒ€": ["2ë£¨íƒ€", "2B", "ì´ë£¨íƒ€"],
            "3ë£¨íƒ€": ["3ë£¨íƒ€", "3B", "ì‚¼ë£¨íƒ€"],
            
            # --- íˆ¬ìˆ˜ ê¸°ë¡ ---
            "í‰ê· ìì±…ì ": ["í‰ê· ìì±…ì ", "í‰ê· ìì±…", "ìì±…ì ", "ë°©ì–´ìœ¨", "ERA"],
            "ìŠ¹ë¦¬": ["ìŠ¹ë¦¬", "ìŠ¹", "ìŠ¹ìˆ˜", "W", "ë‹¤ìŠ¹"],
            "íŒ¨ë°°": ["íŒ¨ë°°", "íŒ¨", "íŒ¨ìˆ˜", "L"],
            "ì„¸ì´ë¸Œ": ["ì„¸ì´ë¸Œ", "SV", "S", "ë’·ë¬¸"],
            "í™€ë“œ": ["í™€ë“œ", "HLD"],
            "ì‚¼ì§„": ["ì‚¼ì§„", "íƒˆì‚¼ì§„", "SO", "K", "ì‚¼ì§„ì•„ì›ƒ"],
            
            # --- ê³µí†µ ---
            "ê²½ê¸°": ["ê²½ê¸°", "ê²Œì„", "G", "ì¶œì¥", "ë“±íŒ"]
        }

        # ------------------------------------------------------------------
        # 3. ë³„ëª… ìë™ ë“±ë¡ ë¡œì§ (ë°ì´í„°ì— ìˆëŠ”ë° ë³„ëª…ì´ ì—†ìœ¼ë©´ ìë™ ìƒì„±)
        # ------------------------------------------------------------------
        for name, stats in self.PLAYERS_DATA.items():
            if name not in self.PLAYER_ALIASES:
                team = stats.get("íŒ€", "")
                # ì˜ˆ: "ê¹€ë„ì˜" -> ["ê¹€ë„ì˜", "ê¸°ì•„ ê¹€ë„ì˜"]
                self.PLAYER_ALIASES[name] = [name, f"{team} {name}"]
        
        print(f"âœ“ ê²€ìƒ‰ì–´(ë³„ëª…) ì¤€ë¹„ ì™„ë£Œ: {len(self.PLAYER_ALIASES)}ê°œ")
        
        # ------------------------------------------------------------------
        # ğŸ”´ [ìˆ˜ì •ëœ ë¶€ë¶„ ë]
        # ------------------------------------------------------------------
    
    def transcribe_audio(self, audio_bytes: bytes) -> Optional[str]:
        """ì˜¤ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (STT) - ë…¸ì´ì¦ˆ í™˜ê²½ ìµœì í™”"""
        if not STT_AVAILABLE or not self.whisper_model:
            print("âœ— STT ë¶ˆê°€: Whisper ëª¨ë¸ ì—†ìŒ")
            return None
        
        temp_path = None
        try:
            # ë…¸ì´ì¦ˆ ì œê±° (ì„¤ì¹˜ëœ ê²½ìš°)
            # processed_audio = reduce_noise_from_wav(audio_bytes) # ê¸°ì¡´ ë…¸ì´ì¦ˆ ì œê±° ë¡œì§ ì œê±°
            
            # ì„ì‹œ íŒŒì¼ ìƒì„±
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as f:
                f.write(audio_bytes)
                temp_path = f.name
            
            # Whisper ë³€í™˜ (ê· í˜•ì¡íŒ ì„¤ì •)
            print("â†’ STT ì²˜ë¦¬ ì¤‘...")
            segments, info = self.whisper_model.transcribe(
                temp_path,
                language="ko",
                beam_size=5,              # ë” ì •í™•í•œ ë””ì½”ë”©
                best_of=5,                # ìµœìƒì˜ ê²°ê³¼ ì„ íƒ
                temperature=0.0,          # í™•ì •ì  ê²°ê³¼ (ëœë¤ì„± ì œê±°)
                vad_filter=True,          # ìŒì„± êµ¬ê°„ë§Œ ê°ì§€
                vad_parameters={
                    "threshold": 0.3,                # ìŒì„± ê°ì§€ ì„ê³„ê°’ (ê´€ëŒ€í•˜ê²Œ)
                    "min_speech_duration_ms": 100,   # ìµœì†Œ ìŒì„± ê¸¸ì´ (ì§§ì€ ë§ë„ ì¸ì‹)
                    "max_speech_duration_s": 30,     # ìµœëŒ€ ìŒì„± ê¸¸ì´
                    "min_silence_duration_ms": 300,  # ìµœì†Œ ë¬µìŒ ê¸¸ì´
                    "speech_pad_ms": 300             # ìŒì„± ì•ë’¤ ì—¬ìœ 
                },
                condition_on_previous_text=False,  # ì´ì „ í…ìŠ¤íŠ¸ ì˜í–¥ ì œê±°
                compression_ratio_threshold=2.4,   # ë°˜ë³µ/ì“°ë ˆê¸° í…ìŠ¤íŠ¸ ì œê±°
                log_prob_threshold=-1.0,           # ë‚®ì€ í™•ë¥  ì„¸ê·¸ë¨¼íŠ¸ ì œê±°
                no_speech_threshold=0.4            # ìŒì„± ì—†ìŒ ì„ê³„ê°’ (ê´€ëŒ€í•˜ê²Œ)
            )
            
            # ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ í™•ì¸
            segments_list = list(segments)
            print(f"  ê°ì§€ëœ ì„¸ê·¸ë¨¼íŠ¸: {len(segments_list)}ê°œ")
            
            if not segments_list:
                print("âœ— STT ê²°ê³¼ ì—†ìŒ (ìŒì„± êµ¬ê°„ ë¯¸ê°ì§€)")
                print("  â†’ ë” í¬ê²Œ ë§ì”€í•´ ì£¼ì„¸ìš”")
                return None
            
            # í…ìŠ¤íŠ¸ ì¡°ë¦½
            text = " ".join(s.text.strip() for s in segments_list).strip().lower()
            
            # ë¹ˆ ê²°ê³¼ ì²´í¬ (ë” ê´€ëŒ€í•˜ê²Œ)
            if not text or len(text) < 1:
                print("âœ— STT ê²°ê³¼ ì—†ìŒ")
                return None
            
            print(f"âœ“ STT ê²°ê³¼: '{text}'")
            print(f"  ì–¸ì–´: {info.language} (í™•ë¥ : {info.language_probability:.2%})")
            return text
            
        except Exception as e:
            print(f"âœ— STT ì‹¤íŒ¨: {e}")
            return None
        finally:
            if temp_path and os.path.exists(temp_path):
                try:
                    os.remove(temp_path)
                except:
                    pass
    
    def generate_gemini_response(self, user_query: str) -> str:
        if not self.gemini_model:
            return "Gemini AI ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        weather_data = get_yongin_weather()
        player_data_str = json.dumps(self.PLAYERS_DATA, ensure_ascii=False, indent=2)
        
        prompt = f"""
        ë‹¹ì‹ ì€ KBO ë¦¬ê·¸ ì‚¼ì„± ë¼ì´ì˜¨ì¦ˆì™€ ê¸°ì•„ íƒ€ì´ê±°ì¦ˆ ì„ ìˆ˜ë“¤ì˜ ê¸°ë¡ì— ëŒ€í•´ ë‹µí•˜ê³ , ìš©ì¸ì˜ í˜„ì¬ ë‚ ì”¨ë¥¼ ì•Œë ¤ì£¼ëŠ” ì¹œì ˆí•œ AI ì•¼êµ¬ ë¹„ì„œì…ë‹ˆë‹¤.

        # ì§€ì¹¨:
        1. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ, ì¹œì ˆí•˜ê³  ì •ì¤‘í•˜ê²Œ ì¡´ëŒ“ë§ë¡œ ë‹µë³€í•˜ì„¸ìš”.  # 1119 ë‹µë³€ ì´ìƒí•´ì„œ ìˆ˜ì •
        2. ë‹µë³€ì€ 1-2 ë¬¸ì¥ìœ¼ë¡œ ì§§ê²Œ ìœ ì§€í•˜ì„¸ìš”.
        3. ì„ ìˆ˜ ê¸°ë¡ ì§ˆë¬¸ì€ ì•„ë˜ `ì„ ìˆ˜ ê¸°ë¡ ë°ì´í„°`ë¥¼ ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µí•˜ì„¸ìš”. ë°ì´í„°ì— ì—†ëŠ” ì„ ìˆ˜ëŠ” "ì£„ì†¡í•´ìš”, ê·¸ ì„ ìˆ˜ì˜ ì •ë³´ëŠ” ì•„ì§ ì—†ì–´ìš”."ë¼ê³  ë‹µí•˜ì„¸ìš”.
        4. ë‚ ì”¨ ì§ˆë¬¸ì€ ì•„ë˜ `ì‹¤ì‹œê°„ ìš©ì¸ ë‚ ì”¨` ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”. ë‚ ì”¨ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ "ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì—ˆì–´ìš”."ë¼ê³  ë‹µí•˜ì„¸ìš”.
        5. ëŒ€í™”ì˜ ë§¥ë½ê³¼ ìƒê´€ì—†ëŠ” ì¼ë°˜ì ì¸ ì§ˆë¬¸ì—ëŠ” "ì €ëŠ” ì•¼êµ¬ ì „ë¬¸ ë¹„ì„œì˜ˆìš”."ë¼ê³  ë‹µí•˜ì„¸ìš”.
        6. ìŒì„±ì´ ì˜¤ì¸ì‹ ë  ìˆ˜ ìˆìœ¼ë‹ˆ ë¹„ìŠ·í•œ ì´ë¦„ì˜ ì„ ìˆ˜, ê¸°ëŠ¥ì„ ìƒê°í•´ì„œ ë‹µí•´ì£¼ì„¸ìš”. ex) ë‚  ì‰¬ì—ˆëŒ€ -> ë‚ ì”¨ì–´ë•Œ, ê¹€ì§„ì°¬ -> ê¹€ì§€ì°¬ ë“±

        ---
        # ì„ ìˆ˜ ê¸°ë¡ ë°ì´í„° (JSON):
        {player_data_str}
        ---
        # ì‹¤ì‹œê°„ ìš©ì¸ ë‚ ì”¨:
        {json.dumps(weather_data, ensure_ascii=False, indent=2) if weather_data else "ë°ì´í„° ì—†ìŒ"}
        ---

        # ì‚¬ìš©ì ì§ˆë¬¸:
        "{user_query}"

        # AI ë‹µë³€:
        """
        
        try:
            print("â†’ Gemini ì‘ë‹µ ìƒì„± ì¤‘...")
            response = self.gemini_model.generate_content(prompt)
            reply = response.text.strip()
            print(f"âœ“ Gemini ì‘ë‹µ: {reply}")
            return reply
        except Exception as e:
            print(f"âœ— Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return "AI ì‘ë‹µ ìƒì„±ì— ì‹¤íŒ¨í–ˆì–´ìš”."

    def process_audio(self, audio_file) -> Dict[str, Any]:
        """ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
        start_time = time.time()
        
        # ì´ˆê¸°í™”
        user_text = None
        reply_text = None
        audio_base64 = None
        display_text = "..."
        
        # í•„ìˆ˜ ëª¨ë“ˆ ì²´í¬
        if not STT_AVAILABLE:
            reply_text = "STT ëª¨ë“ˆ(Faster Whisper)ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        elif not FFMPEG_AVAILABLE:
            reply_text = "ffmpegê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œ: https://www.gyan.dev/ffmpeg/builds/"
        else:
            try:
                # 1. ì˜¤ë””ì˜¤ ë””ì½”ë”© (webm â†’ wav)
                print("â†’ ì˜¤ë””ì˜¤ ë””ì½”ë”© ì¤‘...")
                t1 = time.time()
                
                # ì—…ë¡œë“œëœ íŒŒì¼ì„ ë°”ì´íŠ¸ë¡œ ì½ê¸°
                audio_file.seek(0)
                input_bytes = audio_file.read()
                
                # ffmpegë¡œ ë³€í™˜
                wav_bytes = convert_audio_to_wav(input_bytes)
                
                if not wav_bytes:
                    raise RuntimeError("ì˜¤ë””ì˜¤ ë³€í™˜ ì‹¤íŒ¨")
                    
                print(f"âœ“ ë””ì½”ë”© ì™„ë£Œ ({time.time()-t1:.2f}s)")
                
                # 2. STT
                t2 = time.time()
                user_text = self.transcribe_audio(wav_bytes)
                print(f"âœ“ STT ì™„ë£Œ ({time.time()-t2:.2f}s)")
                
                # 3. NLU
                # player = self.find_player(user_text) if user_text else None # ê¸°ì¡´ í”Œë ˆì´ì–´ ì°¾ê¸° ë¡œì§ ì œê±°
                # keyword = self.find_keyword(user_text) if user_text else None # ê¸°ì¡´ í‚¤ì›Œë“œ ì°¾ê¸° ë¡œì§ ì œê±°
                
                # 4. í…ìŠ¤íŠ¸ ë³´ì •
                # if player and keyword: # ê¸°ì¡´ í…ìŠ¤íŠ¸ ë³´ì • ë¡œì§ ì œê±°
                #     display_text = f"{player} ì„ ìˆ˜ {keyword} ì•Œë ¤ì¤˜"
                # elif user_text:
                #     display_text = user_text
                # else:
                #     display_text = "ìŒì„± ì¸ì‹ ì‹¤íŒ¨"
                
                # 5. ì‘ë‹µ ìƒì„±
                reply_text = self.generate_gemini_response(user_text)
                
            except Exception as e:
                print(f"âœ— ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
                reply_text = f"ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        
        # 6. TTS
        if reply_text:
            t3 = time.time()
            audio_base64 = get_tts_audio(reply_text)
            print(f"âœ“ TTS ì™„ë£Œ ({time.time()-t3:.2f}s)")
        
        total_time = time.time() - start_time
        print(f"âœ“ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}s")
        
        return {
            "ok": True,
            "display_user_text": display_text,
            "reply_text": reply_text,
            "audio_base64": audio_base64
        }

# ============================================================================
# ì‹±ê¸€í†¤ ë° Blueprint
# ============================================================================

_assistant: Optional[VoiceAssistant] = None
def get_assistant() -> VoiceAssistant:
    global _assistant
    if _assistant is None: _assistant = VoiceAssistant()
    return _assistant

voice_bp = Blueprint("voice", __name__)
@voice_bp.route("/api/voice/process_ptt", methods=["POST"])
def api_process_ptt():
    assistant = get_assistant()
    audio_file = request.files.get('audio')
    if not audio_file:
        return jsonify({"ok": False, "error": "ì˜¤ë””ì˜¤ íŒŒì¼ ì—†ìŒ"}), 400
    
    result = assistant.process_audio(audio_file)
    return jsonify(result)
