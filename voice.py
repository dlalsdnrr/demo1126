from __future__ import annotations

import os
import threading
import io
import base64
from typing import Optional, Dict, Any
import time
import difflib

from flask import Blueprint, jsonify, request

# --- STT (Faster Whisper) ---
try:
    from faster_whisper import WhisperModel
    import numpy as np
except Exception: # pragma: no cover
    WhisperModel = None
    np = None
    print("Warning: faster-whisper or numpy not installed. Voice input unavailable.")

# --- TTS (edge-tts + pydub) í†µí•© ---
try:
    import edge_tts
    import asyncio
    from pydub import AudioSegment
    from pydub.effects import speedup
    TTS_AVAILABLE = True
    USE_EDGE_TTS = True
    print("--- INFO: edge-tts module loaded.")
except Exception: # pragma: no cover
    edge_tts = None
    AudioSegment = None
    speedup = None
    TTS_AVAILABLE = False
    USE_EDGE_TTS = False
    print("Warning: edge-tts, pydub or FFmpeg not installed. Audio processing/TTS unavailable.")
    
# Whisper ëª¨ë¸ì€ "base" ëª¨ë¸ ì„¤ì •ì„ ìœ ì§€í•˜ì—¬ ì†ë„ì™€ ì •í™•ë„ì˜ ê· í˜•ì„ ë§ì¶¥ë‹ˆë‹¤.
WHISPER_MODEL = None

def load_whisper_model():
    """Faster Whisper ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    global WHISPER_MODEL
    if WHISPER_MODEL is None and WhisperModel is not None:
        try:
            WHISPER_MODEL = WhisperModel("base", device="cpu", compute_type="int8", cpu_threads=4)
            print("--- INFO: Faster Whisper 'base' model loaded successfully.")
        except Exception as e:
            print(f"--- ERROR: Failed to load Whisper model: {e}")
            pass
    return WHISPER_MODEL

async def speak_edge_tts_to_base64(text: str, voice="ko-KR-SunHiNeural", speed_factor=1.1) -> Optional[str]:
    """edge-ttsë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ê³  Base64 MP3ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not USE_EDGE_TTS or not AudioSegment:
        print("--- ERROR: edge-tts or pydub not available.")
        return None
    
    print(f"--- INFO: TTS generation (edge-tts) for: {text[:30]}...")
    
    try:
        communicate = edge_tts.Communicate(text, voice)
        
        audio_data = b""
        async for chunk in communicate.stream():
            if chunk["type"] == "audio":
                audio_data += chunk["data"]
        
        audio_io = io.BytesIO(audio_data)
        song = AudioSegment.from_mp3(audio_io)
        
        if speed_factor != 1.0:
            song = speedup(song, playback_speed=speed_factor)
        
        output_buffer = io.BytesIO()
        song.export(output_buffer, format="mp3") 
        output_buffer.seek(0)
        
        return base64.b64encode(output_buffer.read()).decode('utf-8')
        
    except Exception as e:
        print(f"--- ERROR: edge-tts failed: {e}")
        return None

def get_tts_base64(text: str) -> Optional[str]:
    """asyncio.runì„ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸° TTS í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
    return loop.run_until_complete(speak_edge_tts_to_base64(text))


class VoiceAssistant:
    def __init__(self) -> None:
        self._whisper_model = load_whisper_model()

        self._exit_keywords = []
        
        # ğŸ’¡ [ê°œì„ ] í‚¤ì›Œë“œ ìœ ì‚¬ì–´ë¥¼ ìµœëŒ€í•œ ë§ì´ ì¶”ê°€
        self.KEYWORDS = {
            "íƒ€ìœ¨": ["íƒ€ìœ¨", "íƒ€ì´ìœ¨", "íƒ€ìœ ìœ¨", "íƒ€ìœ„", "íƒ€ì´ìœ„", "íƒ€ìœ ", "ë‹¤ìœ¨", "íƒ€ë‰¼", "íƒ€ë£°", "íƒ€ìœ ë¥¼", "íƒ€ìœ ë¦¬", "íƒ€ìœ¨ì€", "íƒ€ìœ¨ì´", 
                     "ë‹¤ìš”ë˜", "íƒ€ì´ìœ ", "íƒ€ìš”ë¥¼", "íƒ€ìš”ìœ¨", "ë‹¤ìœ¡", "ë‹¤ì´ìœ¨", "ë‹¤ì´ìœ ", "ë‹¤ìœ "],
            "í™ˆëŸ°": ["í™ˆëŸ°", "í™ëŸ°", "í™ˆë¡¬", "í™ë¡ ", "í›”ëŠ”", "í™ˆë¡ ", "í™ˆëˆˆ", "í—˜ë¡ ", "í˜¸ë„ˆ", "í™ˆë„ˆ", "í™ˆë„Œ", "í™ˆëŸ°ì€", "í™ˆëŸ°ì´", "í™ˆëŸ°ê°œìˆ˜",
                     "í™ë‚¨", "í™ˆë‚¨", "í™ëŸ¼", "í™ˆë„˜", "í ëŸ°", "ìŒë€", "ì—„ë‚¨"],
            "ì•ˆíƒ€": ["ì•ˆíƒ€", "ì•™íƒ€", "ì•ˆ íƒ€", "ì•”íƒ€", "ì•ˆíƒˆ", "ì•ˆíƒ‘", "ì•„íƒ€", "ì•ˆíƒ€ëŠ”", "ì•ˆíƒ€ê°€", "ì•„ì•ˆíƒ€", "ì•ˆíƒ€ê°œìˆ˜",
                     "ì•ˆë‚˜", "ì•ˆíƒ€ë¡œ", "ì•ˆë‹¤", "ì•ˆë‹¬", "ì•˜ë‹¤"]
        }
        
        # ğŸ’¡ [ê°œì„ ] ëª¨ë“  ì„ ìˆ˜ ë³„ëª…ì— ì˜¤ì¸ì‹ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë³„ëª… ì¶”ê°€
        self.PLAYER_ALIASES = {
            "ê¹€ì˜ì›…": ["ê¹€ì˜ì›…", "ê¸°ëª…ì›…", "ê¹€í˜•ì›…", "ê¹€ì˜", "ê¸°ëª…", "ê¹€ìš©ì›…", "ê¹€ì—¬ìš´", "ê¹€ì˜ì›…ì´", "ê¹€ì´ìš©", "ê¹€ì´ì›…", "ì´ëª…ìš°", "ê¹€ì—¬ë¦„"],
            "ë¦¬ë² ë¼í† ": ["ë¦¬ë² ë¼í† ", "ì´ë² ë¼í† ", "ë¦¬ë² ë¼", "ì´ë² ë¼", "ì´ë² ë¼ë„", "ë¦¬ë² ë¼í† ëŠ”", "ë¦¬ë² ë¼í† ì˜", "ë¦¬ë°°ë¼í† ", "ë‹ˆë² ë¼ë„", "ì´ë² ë¼ë„", "ë¦¬ë² ë¼ë„"],
            "í•˜ì£¼ì„": ["í•˜ì£¼ì„", "ì•„ì£¼ì„", "í™”ì£¼ì„", "í•˜ì£¼ì†Œ", "í•˜ì£¼", "í•˜ì£¼ì„ì´", "í•˜ì£¼ì„ì€", "í•˜ì¦ˆì„", "í•˜ë‚˜ íˆ¬ì†", "í•˜ë‚˜íˆ¬ì†", "ì•„ë‚˜ íˆ¬ì†", "ì•„ì£¼ì„"],
            "ê¹€íƒœí›ˆ": ["ê¹€íƒœí›ˆ", "ê¹€íƒœìš´", "ê¹€íƒœí¬", "ê¹€ëŒ€í›ˆ", "ê¹€ëŒ€ìš´", "ê¹€íƒœí›ˆì´", "ê¹€íƒœí›ˆì€", "ê¹€ëŒ€í›ˆì´", "ê¹€íƒœìš°", "ê¹€íƒœ", "ê¹€ëŒ€ìš´"],
            "ìµœì¬í›ˆ": ["ìµœì¬í›ˆ", "ì²´ì¬í›ˆ", "ì·Œì¬í›ˆ", "ìµœì¬", "ìµœì¬í›ˆì´", "ìµœì¬í›ˆì€", "ìµœëŒ€í›ˆ", "ìµœì •ì€", "ì±„ì¬í›ˆ", "ì²´ì œí›ˆ", "ìµœì €ì˜¨"]
        }
        
        self.PLAYERS_DATA = {
            "ê¹€ì˜ì›…": { "íƒ€ìœ¨": 0.643, "í™ˆëŸ°": 3, "ì•ˆíƒ€": 9 },
            "ë¦¬ë² ë¼í† ": { "íƒ€ìœ¨": 0.467, "í™ˆëŸ°": 1, "ì•ˆíƒ€": 7 },
            "í•˜ì£¼ì„": { "íƒ€ìœ¨": 0.438, "í™ˆëŸ°": 0, "ì•ˆíƒ€": 7 },
            "ê¹€íƒœí›ˆ": { "íƒ€ìœ¨": 0.429, "í™ˆëŸ°": 2, "ì•ˆíƒ€": 6 },
            "ìµœì¬í›ˆ": { "íƒ€ìœ¨": 0.385, "í™ˆëŸ°": 0, "ì•ˆíƒ€": 5 }
        }

    # --- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ---
    def _fuzzy_match(self, text: str, candidates: list[str], threshold=0.65) -> bool: # ì„ê³„ê°’ 0.65ë¡œ ìœ ì§€
        """í¼ì§€ ë§¤ì¹­ì„ í†µí•´ í…ìŠ¤íŠ¸ì™€ í›„ë³´ ë‹¨ì–´ë¥¼ ë¹„êµí•©ë‹ˆë‹¤."""
        for word in candidates:
            if word in text:
                return True
        for candidate in candidates:
            if difflib.SequenceMatcher(None, text, candidate).ratio() > threshold:
                return True
        return False

    def _transcribe(self, audio: np.ndarray) -> Optional[str]:
        if self._whisper_model is None:
            return None
        try:
            # beam_size=5 ìœ ì§€
            segments, _ = self._whisper_model.transcribe(
                audio, language="ko", beam_size=5, best_of=5,
                vad_filter=True, vad_parameters={"min_silence_duration_ms": 500}
            )
            text = " ".join(segment.text.strip() for segment in segments).lower() 
            return text if text else None
        except Exception as e:
            print(f"--- ERROR: Transcription failed: {e}")
            return None

    def _find_player(self, text: str) -> Optional[str]:
        if not text: return None
        for canonical_name, aliases in self.PLAYER_ALIASES.items():
            if self._fuzzy_match(text, aliases):
                return canonical_name
        return None

    def _find_keyword(self, text: str) -> Optional[str]:
        if not text: return None
        
        # ğŸ’¡ [ê°œì„ ] 'ë‹¤ìš”ë˜'ê°€ ë°œê²¬ë˜ë©´ 'íƒ€ìœ¨'ë¡œ ê°•ì œ ë§¤í•‘ (ì´ì „ ì˜¤ë¥˜ íŒ¨í„´ ë°˜ì˜)
        if "ë‹¤ìš”ë˜" in text:
            return "íƒ€ìœ¨"
            
        for keyword, similar_words in self.KEYWORDS.items():
            if self._fuzzy_match(text, similar_words):
                return keyword
        return None

    def _get_reply(self, text: str, player_name: Optional[str], keyword: Optional[str]) -> str:
        if not text:
            return "ì˜ ëª» ë“¤ì—ˆì–´ìš”. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"
        
        if not player_name:
            return "ì£„ì†¡í•´ìš”, ì„ ìˆ˜ ì´ë¦„ì„ ë§ì”€í•´ì£¼ì„¸ìš”."
        if not keyword:
            return f"{player_name} ì„ ìˆ˜ì˜ ì–´ë–¤ ì •ë³´ê°€ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?"
            
        player_info = self.PLAYERS_DATA.get(player_name)
        if player_info is None:
            return f"ì£„ì†¡í•´ìš”, {player_name} ì„ ìˆ˜ì˜ ê¸°ë¡ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
            
        value = player_info.get(keyword)
        if value is None:
            return f"ì£„ì†¡í•´ìš”, {player_name} ì„ ìˆ˜ì˜ {keyword} ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        if keyword == "íƒ€ìœ¨":
            return f"{player_name} ì„ ìˆ˜ì˜ íƒ€ìœ¨ì€ {value:.3f}ì…ë‹ˆë‹¤."
        elif keyword == "í™ˆëŸ°":
            return f"{player_name} ì„ ìˆ˜ì˜ í™ˆëŸ°ì€ {value}ê°œì…ë‹ˆë‹¤."
        elif keyword == "ì•ˆíƒ€":
            return f"{player_name} ì„ ìˆ˜ì˜ ì•ˆíƒ€ëŠ” {value}ê°œì…ë‹ˆë‹¤."
        else:
            return f"{player_name} ì„ ìˆ˜ì˜ {keyword}ì€(ëŠ”) {value}ì…ë‹ˆë‹¤."
            
    def process_ptt_audio(self, audio_file_storage) -> Dict[str, Any]:
        """PTT ì˜¤ë””ì˜¤ë¥¼ ì²˜ë¦¬í•˜ê³ , í…ìŠ¤íŠ¸ì™€ Base64 ì˜¤ë””ì˜¤ê°€ í¬í•¨ëœ JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        start_time = time.time()

        user_text = None
        reply_text = None
        player_name = None
        keyword = None
        display_user_text = "..."
        audio_base64 = None
            
        if self._whisper_model is None or not np or not TTS_AVAILABLE:
            reply_text = "ìŒì„± ì²˜ë¦¬ ëª¨ë“ˆ(Whisper/Pydub/Edge-TTS)ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        else:
            try:
                # --- 1. ì˜¤ë””ì˜¤ ë¡œë“œ ë° ë³€í™˜ (pydub) ---
                load_start = time.time()
                audio_segment = AudioSegment.from_file(audio_file_storage)
                audio_segment = audio_segment.set_frame_rate(16000).set_channels(1).set_sample_width(2)
                samples = np.array(audio_segment.get_array_of_samples())
                audio_float = samples.astype(np.float32) / 32768.0
                audio_to_transcribe = audio_float
                print(f"--- TIME: Audio Load/Convert: {time.time() - load_start:.3f}s")
                
                # --- 2. STT (ìŒì„± -> í…ìŠ¤íŠ¸) ---
                stt_start = time.time()
                user_text = self._transcribe(audio_to_transcribe)
                print(f"--- TIME: STT Transcription: {time.time() - stt_start:.3f}s")
                print(f"--- INFO: STT Text: {user_text}")

                # --- 3. NLU (í…ìŠ¤íŠ¸ -> ì˜ë„) ---
                nlu_start = time.time()
                if user_text:
                    player_name = self._find_player(user_text)
                    keyword = self._find_keyword(user_text)
                print(f"--- TIME: NLU Processing: {time.time() - nlu_start:.3f}s")

                # --- 4. í…ìŠ¤íŠ¸ ë³´ì • ---
                if player_name and keyword:
                    display_user_text = f"{player_name} ì„ ìˆ˜ {keyword} ì•Œë ¤ì¤˜"
                elif user_text:
                    display_user_text = user_text
                
                # --- 5. ì‘ë‹µ ìƒì„± ---
                reply_text = self._get_reply(user_text, player_name, keyword)
                
            except Exception as e:
                print(f"--- ERROR: Failed to process PTT audio: {e}")
                reply_text = "ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

        # --- 6. TTS (AI í…ìŠ¤íŠ¸ -> AI ìŒì„±) ë° Base64 ì¸ì½”ë”© ---
        tts_start = time.time()
        if TTS_AVAILABLE:
            audio_base64 = get_tts_base64(reply_text)
        else:
            audio_base64 = None
            print("--- WARNING: TTS is not available, skipping audio generation.")
            
        print(f"--- TIME: TTS Generation: {time.time() - tts_start:.3f}s")
        
        total_time = time.time() - start_time
        print(f"--- TIME: Total process time: {total_time:.3f}s")
            
        # --- 7. ìµœì¢… JSON ë°˜í™˜ ---
        return {
            "ok": True,
            "display_user_text": display_user_text,
            "reply_text": reply_text,
            "audio_base64": audio_base64 # Base64 ì˜¤ë””ì˜¤ ë°ì´í„°
        }


# --- ì‹±ê¸€í†¤ ë° Blueprint (ë³€ê²½ ì—†ìŒ) ---
_singleton: Optional[VoiceAssistant] = None

def get_assistant() -> VoiceAssistant:
    """VoiceAssistant ì‹±ê¸€í†¤ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    global _singleton
    if _singleton is None:
        _singleton = VoiceAssistant()
    return _singleton


voice_bp = Blueprint("voice", __name__)


@voice_bp.route("/api/voice/process_ptt", methods=["POST"])
def api_voice_process_ptt():
    """PTT ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë°›ì•„ ì²˜ë¦¬í•˜ê³  JSON ì‘ë‹µì„ ë°˜í™˜í•˜ëŠ” API"""
    va = get_assistant()
    
    audio_file = request.files.get('audio')
    if not audio_file:
        return jsonify({"ok": False, "error": "No audio file provided"}), 400

    response_data = va.process_ptt_audio(audio_file)

    if not response_data.get("ok"):
            return jsonify({"ok": False, "error": "Failed to process audio"}), 500

    return jsonify(response_data)
