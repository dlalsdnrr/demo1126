from __future__ import annotations

import os
import threading
import io
import base64
from typing import Optional, Dict, Any
import time
import difflib
# import subprocess # espeak-ng ì‚¬ìš©ì„ ìœ„í•´ ì œê±°

from flask import Blueprint, jsonify, request

# --- STT (Faster Whisper) ---
try:
    from faster_whisper import WhisperModel
    import numpy as np
except Exception: # pragma: no cover
    WhisperModel = None
    np = None
    print("Warning: faster-whisper or numpy not installed. Voice input unavailable.")

# --- TTS (edge-tts + pydub) í†µí•© ---
try:
    import edge_tts
    import asyncio
    from pydub import AudioSegment
    # from pydub.playback import play # ì›¹ API í™˜ê²½ì—ì„œëŠ” play ëŒ€ì‹  Base64 ì¸ì½”ë”© ì‚¬ìš©
    from pydub.effects import speedup
    TTS_AVAILABLE = True
    USE_EDGE_TTS = True
    print("--- INFO: edge-tts module loaded.")
except Exception: # pragma: no cover
    edge_tts = None
    AudioSegment = None
    speedup = None
    TTS_AVAILABLE = False
    USE_EDGE_TTS = False
    print("Warning: edge-tts, pydub or FFmpeg not installed. Audio processing/TTS unavailable.")
    
# Whisper ëª¨ë¸ ë¡œë“œëŠ” ë‘ ë²ˆì§¸ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ ìœ ì§€
WHISPER_MODEL = None

def load_whisper_model():
    """Faster Whisper ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    global WHISPER_MODEL
    if WHISPER_MODEL is None and WhisperModel is not None:
        try:
            # ë‘ ë²ˆì§¸ ì½”ë“œì˜ ì„¤ì • ("base" ëª¨ë¸) ìœ ì§€
            WHISPER_MODEL = WhisperModel("base", device="cpu", compute_type="int8", cpu_threads=4)
            print("--- INFO: Faster Whisper 'base' model loaded successfully.")
        except Exception as e:
            print(f"--- ERROR: Failed to load Whisper model: {e}")
            pass
    return WHISPER_MODEL

# ğŸ’¡ [TTS ë³€ê²½] espeak-ng ëŒ€ì‹  edge-ttsë¥¼ ì‚¬ìš©í•˜ë©°, ê²°ê³¼ë¥¼ Base64 WAV/MP3ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
async def speak_edge_tts_to_base64(text: str, voice="ko-KR-SunHiNeural", speed_factor=1.1) -> Optional[str]:
    """edge-ttsë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ê³  Base64 MP3ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not USE_EDGE_TTS or not AudioSegment:
        print("--- ERROR: edge-tts or pydub not available.")
        return None
    
    print(f"--- INFO: TTS generation (edge-tts) for: {text[:30]}...")
    
    try:
        communicate = edge_tts.Communicate(text, voice)
        
        # 1. ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ ë²„í¼ì— ì €ì¥
        audio_data = b""
        async for chunk in communicate.stream():
            if chunk["type"] == "audio":
                audio_data += chunk["data"]
        
        # 2. pydubìœ¼ë¡œ MP3 ë¡œë“œ ë° ì†ë„ ì¡°ì ˆ
        audio_io = io.BytesIO(audio_data)
        song = AudioSegment.from_mp3(audio_io)
        
        if speed_factor != 1.0:
            # from pydub.effects import speedup (speedup í•¨ìˆ˜ ì‚¬ìš©)
            # 1.1ë°°ì†ìœ¼ë¡œ ë¹ ë¥´ê²Œ (ì„¸ ë²ˆì§¸ ì½”ë“œì˜ ì„¤ì • ì ìš©)
            song = speedup(song, playback_speed=speed_factor)
        
        # 3. ì˜¤ë””ì˜¤ë¥¼ ë©”ëª¨ë¦¬ì— WAV ë˜ëŠ” MP3ë¡œ ì¸ì½”ë”© (ì›¹ í™˜ê²½ì— ë§ê²Œ MP3/WAV ì„ íƒ ê°€ëŠ¥)
        output_buffer = io.BytesIO()
        # MP3ë¡œ ì¸ì½”ë”© (Base64 í¬ê¸°ë¥¼ ì¤„ì´ê¸° ìœ„í•´)
        song.export(output_buffer, format="mp3") 
        output_buffer.seek(0)
        
        # 4. Base64 ì¸ì½”ë”© ë° ë°˜í™˜
        return base64.b64encode(output_buffer.read()).decode('utf-8')
        
    except Exception as e:
        print(f"--- ERROR: edge-tts failed: {e}")
        return None

# ğŸ’¡ ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ë™ê¸°ë¡œ ê°ì‹¸ëŠ” í—¬í¼ í•¨ìˆ˜ (Flask APIëŠ” ë™ê¸°ì ì´ë¯€ë¡œ í•„ìš”)
def get_tts_base64(text: str) -> Optional[str]:
    """asyncio.runì„ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸° TTS í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    # ìœˆë„ìš° í™˜ê²½ì—ì„œ asyncio.runì„ ìŠ¤ë ˆë“œ ë‚´ì—ì„œ í˜¸ì¶œí•  ë•Œì˜ ì˜¤ë¥˜ë¥¼ ë°©ì§€
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
    return loop.run_until_complete(speak_edge_tts_to_base64(text))


class VoiceAssistant:
    def __init__(self) -> None:
        self._whisper_model = load_whisper_model()

        # ë‘ ë²ˆì§¸ ì½”ë“œì˜ í‚¤ì›Œë“œ ë° ë°ì´í„° ìœ ì§€
        self._exit_keywords = [
            "ì¢…ë£Œ", "ê·¸ë§Œ", "ëŒ€í™” ì¢…ë£Œ", "ë", "ë‚˜ê°€ê¸°",
            "ì¢…ë£Œí•´", "ì¢…ë£Œìš”", "ì´ì œ ê·¸ë§Œ", "ì¢…ë‡¨","ì¢…ìš”", "ì´ì œëì–´"
        ]
        self.KEYWORDS = {
            "íƒ€ìœ¨": ["íƒ€ìœ¨", "íƒ€ì´ìœ¨", "íƒ€ìœ ìœ¨", "íƒ€ìœ„", "íƒ€ì´ìœ„", "íƒ€ìœ ", "ë‹¤ìœ¨", "íƒ€ë‰¼", "íƒ€ë£°", "íƒ€ìœ ë¥¼", "íƒ€ìœ ë¦¬", "íƒ€ìœ¨ì€", "íƒ€ìœ¨ì´"],
            "í™ˆëŸ°": ["í™ˆëŸ°", "í™ëŸ°", "í™ˆë¡¬", "í™ë¡ ", "í›”ëŠ”", "í™ˆë¡ ", "í™ˆëˆˆ", "í—˜ë¡ ", "í˜¸ë„ˆ", "í™ˆë„ˆ", "í™ˆë„Œ", "í™ˆëŸ°ì€", "í™ˆëŸ°ì´", "í™ˆëŸ°ê°œìˆ˜"],
            "ì•ˆíƒ€": ["ì•ˆíƒ€", "ì•™íƒ€", "ì•ˆ íƒ€", "ì•”íƒ€", "ì•ˆíƒˆ", "ì•ˆíƒ‘", "ì•„íƒ€", "ì•ˆíƒ€ëŠ”", "ì•ˆíƒ€ê°€", "ì•„ì•ˆíƒ€", "ì•ˆíƒ€ê°œìˆ˜"]
        }
        self.PLAYER_ALIASES = {
            "ê¹€ì§€ì°¬": ["ê¹€ì§€ì°¬", "ê¹€ì§€ì°½", "ê¹€ì§€ì°¨", "ê¹€ì§€ì°¨ë‹ˆ", "ê¹€ì§€ì°¬ì´", "ê¹€ì§€ì²­", "ê¹€ì§€ì°¨ëŠ”", "ê¹€ì§€ì°¬ì€", "ê¸°ì§€ì°¬", "ê¹€ì§€ì°¾"],
            "êµ¬ììš±": ["êµ¬ììš±", "êµ¬ìì˜¥", "êµ¬ììš°", "êµ¬ìì˜¤", "êµ¬ììš±ì´", "êµ¬ìì˜¤ê¸°", "ê³ ììš±", "êµ¬ììš±ì€", "êµ¬ììš°ê¸°", "êµ¬ììš´", "êµ¬ìêµ¬"],
            "ë¥˜í˜„ì§„": ["ë¥˜í˜„ì§„", "ìœ í˜„ì§„", "ë‰´í˜„ì§„", "ìœ í˜„ì‹ ", "ë¥˜í˜„ì‹ ", "ìœ í˜„ì§€", "ë¥˜í˜„ì§€", "ìœ í˜„ì§€ëŠ”", "ë¥˜í˜„ì§€ëŠ”", "ìœ í˜„ì§€ë‹ˆ", "ë£¨í˜„ì§„"]
        }
        self.PLAYERS_DATA = {
            "ê¹€ì§€ì°¬": { "íƒ€ìœ¨": 0.285, "í™ˆëŸ°": 1, "ì•ˆíƒ€": 80 },
            "êµ¬ììš±": { "íƒ€ìœ¨": 0.315, "í™ˆëŸ°": 22, "ì•ˆíƒ€": 155 },
            "ë¥˜í˜„ì§„": { "íƒ€ìœ¨": 0.150, "í™ˆëŸ°": 0, "ì•ˆíƒ€": 5 }
        }

    # --- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (ë‘ ë²ˆì§¸ ì½”ë“œì˜ í¼ì§€ ë§¤ì¹­ ë¡œì§ ìœ ì§€) ---
    def _fuzzy_match(self, text: str, candidates: list[str], threshold=0.7) -> bool:
        """í¼ì§€ ë§¤ì¹­ì„ í†µí•´ í…ìŠ¤íŠ¸ì™€ í›„ë³´ ë‹¨ì–´ë¥¼ ë¹„êµí•©ë‹ˆë‹¤."""
        for word in candidates:
            if word in text:
                return True
        for candidate in candidates:
             if difflib.SequenceMatcher(None, text, candidate).ratio() > threshold:
                return True
        return False

    def _transcribe(self, audio: np.ndarray) -> Optional[str]:
        # ë‘ ë²ˆì§¸ ì½”ë“œì˜ STT ë¡œì§ ìœ ì§€ (base ëª¨ë¸, ê³µë°± ìœ ì§€)
        if self._whisper_model is None:
            return None
        try:
            segments, _ = self._whisper_model.transcribe(
                audio, language="ko", beam_size=5, best_of=5,
                vad_filter=True, vad_parameters={"min_silence_duration_ms": 500}
            )
            text = " ".join(segment.text.strip() for segment in segments).lower() 
            return text if text else None
        except Exception as e:
            print(f"--- ERROR: Transcription failed: {e}")
            return None

    def _find_player(self, text: str) -> Optional[str]:
        # ë‘ ë²ˆì§¸ ì½”ë“œì˜ NLU ë¡œì§ ìœ ì§€ (í¼ì§€ ë§¤ì¹­ ì‚¬ìš©)
        if not text: return None
        for canonical_name, aliases in self.PLAYER_ALIASES.items():
            if self._fuzzy_match(text, aliases):
                return canonical_name
        return None

    def _find_keyword(self, text: str) -> Optional[str]:
        # ë‘ ë²ˆì§¸ ì½”ë“œì˜ NLU ë¡œì§ ìœ ì§€ (í¼ì§€ ë§¤ì¹­ ì‚¬ìš©)
        if not text: return None
        for keyword, similar_words in self.KEYWORDS.items():
            if self._fuzzy_match(text, similar_words):
                return keyword
        return None

    def _get_reply(self, text: str, player_name: Optional[str], keyword: Optional[str]) -> str:
        # ë‘ ë²ˆì§¸ ì½”ë“œì˜ ì‘ë‹µ ìƒì„± ë¡œì§ ìœ ì§€
        if not text:
            return "ì˜ ëª» ë“¤ì—ˆì–´ìš”. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"
        if any(self._fuzzy_match(text, [exit_kw]) for exit_kw in self._exit_keywords):
            return "ë„¤. ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤."
        if not player_name:
            return "ì£„ì†¡í•´ìš”, ì„ ìˆ˜ ì´ë¦„ì„ ë§ì”€í•´ì£¼ì„¸ìš”."
        if not keyword:
            return f"{player_name} ì„ ìˆ˜ì˜ ì–´ë–¤ ì •ë³´ê°€ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?"
        player_info = self.PLAYERS_DATA.get(player_name)
        value = player_info.get(keyword)
        if value is None:
            return f"ì£„ì†¡í•´ìš”, {player_name} ì„ ìˆ˜ì˜ {keyword} ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # íƒ€ìœ¨ì€ .3f í¬ë§· ìœ ì§€
        if keyword == "íƒ€ìœ¨":
            return f"{player_name} ì„ ìˆ˜ì˜ íƒ€ìœ¨ì€ {value:.3f}ì…ë‹ˆë‹¤."
        elif keyword == "í™ˆëŸ°":
            return f"{player_name} ì„ ìˆ˜ì˜ í™ˆëŸ°ì€ {value}ê°œì…ë‹ˆë‹¤."
        elif keyword == "ì•ˆíƒ€":
            return f"{player_name} ì„ ìˆ˜ì˜ ì•ˆíƒ€ëŠ” {value}ê°œì…ë‹ˆë‹¤."
        else:
            return f"{player_name} ì„ ìˆ˜ì˜ {keyword}ì€(ëŠ”) {value}ì…ë‹ˆë‹¤."
            
    def process_ptt_audio(self, audio_file_storage) -> Dict[str, Any]:
        """PTT ì˜¤ë””ì˜¤ë¥¼ ì²˜ë¦¬í•˜ê³ , í…ìŠ¤íŠ¸ì™€ Base64 ì˜¤ë””ì˜¤ê°€ í¬í•¨ëœ JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        start_time = time.time()

        user_text = None
        reply_text = None
        player_name = None
        keyword = None
        display_user_text = "..."
        audio_base64 = None
            
        if self._whisper_model is None or not np or not TTS_AVAILABLE:
            reply_text = "ìŒì„± ì²˜ë¦¬ ëª¨ë“ˆ(Whisper/Pydub/Edge-TTS)ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        else:
            try:
                # --- 1. ì˜¤ë””ì˜¤ ë¡œë“œ ë° ë³€í™˜ (pydub) ---
                load_start = time.time()
                audio_segment = AudioSegment.from_file(audio_file_storage)
                audio_segment = audio_segment.set_frame_rate(16000).set_channels(1).set_sample_width(2)
                samples = np.array(audio_segment.get_array_of_samples())
                audio_float = samples.astype(np.float32) / 32768.0
                audio_to_transcribe = audio_float
                print(f"--- TIME: Audio Load/Convert: {time.time() - load_start:.3f}s")
                
                # --- 2. STT (ìŒì„± -> í…ìŠ¤íŠ¸) ---
                stt_start = time.time()
                user_text = self._transcribe(audio_to_transcribe)
                print(f"--- TIME: STT Transcription: {time.time() - stt_start:.3f}s")
                print(f"--- INFO: STT Text: {user_text}")

                # --- 3. NLU (í…ìŠ¤íŠ¸ -> ì˜ë„) ---
                nlu_start = time.time()
                if user_text:
                    player_name = self._find_player(user_text)
                    keyword = self._find_keyword(user_text)
                print(f"--- TIME: NLU Processing: {time.time() - nlu_start:.3f}s")

                # --- 4. í…ìŠ¤íŠ¸ ë³´ì • ---
                if player_name and keyword:
                    # ë‘ ë²ˆì§¸ ì½”ë“œì˜ í‘œì‹œ í…ìŠ¤íŠ¸ í˜•ì‹ ìœ ì§€
                    display_user_text = f"{player_name} ì„ ìˆ˜ {keyword} ì•Œë ¤ì¤˜"
                elif user_text:
                    display_user_text = user_text
                
                # --- 5. ì‘ë‹µ ìƒì„± ---
                reply_text = self._get_reply(user_text, player_name, keyword)
                
            except Exception as e:
                print(f"--- ERROR: Failed to process PTT audio: {e}")
                reply_text = "ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

        # --- 6. TTS (AI í…ìŠ¤íŠ¸ -> AI ìŒì„±) ë° Base64 ì¸ì½”ë”© ---
        tts_start = time.time()
        # ğŸ’¡ [TTS ë³€ê²½] edge-ttsë¡œ ë³€ê²½í•˜ì—¬ Base64 ì˜¤ë””ì˜¤ ìƒì„±
        if TTS_AVAILABLE:
            audio_base64 = get_tts_base64(reply_text)
        else:
             audio_base64 = None
             print("--- WARNING: TTS is not available, skipping audio generation.")
             
        print(f"--- TIME: TTS Generation: {time.time() - tts_start:.3f}s")
        
        total_time = time.time() - start_time
        print(f"--- TIME: Total process time: {total_time:.3f}s")
            
        # --- 7. ìµœì¢… JSON ë°˜í™˜ ---
        return {
            "ok": True,
            "display_user_text": display_user_text,
            "reply_text": reply_text,
            "audio_base64": audio_base64 # Base64 ì˜¤ë””ì˜¤ ë°ì´í„°
        }


# --- ì‹±ê¸€í†¤ ë° Blueprint (ë³€ê²½ ì—†ìŒ) ---
_singleton: Optional[VoiceAssistant] = None

def get_assistant() -> VoiceAssistant:
    """VoiceAssistant ì‹±ê¸€í†¤ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    global _singleton
    if _singleton is None:
        _singleton = VoiceAssistant()
    return _singleton


voice_bp = Blueprint("voice", __name__)


@voice_bp.route("/api/voice/process_ptt", methods=["POST"])
def api_voice_process_ptt():
    """PTT ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë°›ì•„ ì²˜ë¦¬í•˜ê³  JSON ì‘ë‹µì„ ë°˜í™˜í•˜ëŠ” API"""
    va = get_assistant()
    
    audio_file = request.files.get('audio')
    if not audio_file:
        return jsonify({"ok": False, "error": "No audio file provided"}), 400

    response_data = va.process_ptt_audio(audio_file)

    if not response_data.get("ok"):
           return jsonify({"ok": False, "error": "Failed to process audio"}), 500

    return jsonify(response_data)
